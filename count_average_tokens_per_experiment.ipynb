{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25YqnQceIxc_"
   },
   "source": [
    "# Count Average Tokens Generated Per Experiment\n",
    "This is approximated by counting tokens in the code generated by one arbitrary model bsed on the instructional prompt (the medium length promtp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use this section if running in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 12995,
     "status": "ok",
     "timestamp": 1750909549948,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "2Cts4xkGN2Wd",
    "outputId": "417a2399-20ef-40d0-b401-2d50724a1eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7099,
     "status": "ok",
     "timestamp": 1750909563668,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "7pzW2dPsVqFn",
    "outputId": "6b2b0142-1306-4131-9f9b-1c45175e49e0"
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1750909569112,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "rNqDtK8eKbWT",
    "outputId": "3a96b8ae-a943-4173-db86-33502f96328f"
   },
   "outputs": [],
   "source": [
    "# mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')             # Mount Google Drive to Colab session\n",
    "!ls /content/drive/MyDrive                # Verify that your drive is accessible\n",
    "\n",
    "# to be able to import local packages\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/Colab Notebooks/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local code section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 6160,
     "status": "ok",
     "timestamp": 1750909579580,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "9obW8kRvIvEX"
   },
   "outputs": [],
   "source": [
    "from helpers import read_problems, stream_jsonl\n",
    "from prompts import complete_code_prompt, complete_task_prompt\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1750909581347,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "gJmy5Da_I8i6"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "\n",
    "def count_tokens(text):\n",
    "    return len(enc.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4 latency (ms)\n",
    "gpt4_latency_per_token = 0.018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPy7cffPXvur"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g4Y0DxfXyyv"
   },
   "source": [
    "## HumanEval Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1750906647164,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "OR9TM0yaI9Cv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(164,\n",
       " {'task_id': 'HumanEval/0',\n",
       "  'completion': '```python\\nfrom typing import List\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \"\"\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \"\"\"\\n    for i in range(len(numbers)):\\n        for j in range(i+1, len(numbers)):\\n            if abs(numbers[i] - numbers[j]) < threshold:\\n                return True\\n    return False\\n\\nprint(has_close_elements([1.0, 2.0, 3.0], 0.5))\\nprint(has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3))\\n```'})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/Users/andrew/Documents/01_documents/GWU_DENG/SEAS_8588_Praxis/2_Code/logs_results/final-round-one-model/HumanEval/OpenCodeInterpreter-DS-6.7B/HumanEval_OpenCodeInterpreter-DS-6.7B_complete_code_prompt_temperature1.0_topP1.0_completions_20250322_054821_9443.jsonl'\n",
    "results = list( stream_jsonl(file) )\n",
    "len(results), results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750908829761,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "hKReYoj4QIuI",
    "outputId": "933eedbe-ecad-478c-a805-999c0681a221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7708\n",
      "21538\n",
      "Total tokens in prompts for the entire experiment: 29246\n"
     ]
    }
   ],
   "source": [
    "# prompt template without content\n",
    "prompt_template_tokens = count_tokens(complete_code_prompt)*len(results)\n",
    "print(prompt_template_tokens)\n",
    "\n",
    "# prompt content\n",
    "dataset = load_dataset(\"openai/openai_humaneval\")\n",
    "prompt_content_tokens = count_tokens( ' '.join([i['prompt'] for i in dataset[\"test\"]]) )\n",
    "print(prompt_content_tokens)\n",
    "\n",
    "# total prompt tokens (entire experiment)\n",
    "total_prompt_tokens = prompt_template_tokens + prompt_content_tokens\n",
    "print('Total tokens in prompts for the entire experiment:', total_prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1750908535616,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "FLI99SL2Iuvo",
    "outputId": "868cb191-7553-4474-c78e-2e83f5e24b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated tokens for the entire experiment: 29054\n"
     ]
    }
   ],
   "source": [
    "# generated text\n",
    "generated_tokens = sum([count_tokens(i['completion']) for i in results])\n",
    "print('Total generated tokens for the entire experiment:', generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ijEMcpdcOTV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177.15853658536585\n",
      "Average latency per one code generation: 3.188853658536585 seconds\n"
     ]
    }
   ],
   "source": [
    "# average latency\n",
    "avg_tokens_per_one_generation  = generated_tokens/len(results)\n",
    "print( avg_tokens_per_one_generation )\n",
    "avg_latency_per_one_generation = avg_tokens_per_one_generation * gpt4_latency_per_token\n",
    "print(f'Average latency per one code generation: {avg_latency_per_one_generation} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYbotH-pUNSt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g4Y0DxfXyyv"
   },
   "source": [
    "## BigCodeBench Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1750906647164,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "OR9TM0yaI9Cv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,\n",
       " {'task_id': 'BigCodeBench/0',\n",
       "  'completion': '```python\\nimport itertools\\nfrom random import shuffle\\n\\ndef task_func(numbers=list(range(1, 3))):\\n    \"\"\"\\n    Calculates the average of the sums of absolute differences between each pair of consecutive numbers \\n    for all permutations of a given list. Each permutation is shuffled before calculating the differences.\\n\\n    Args:\\n    - numbers (list): A list of numbers. Default is numbers from 1 to 10.\\n    \\n    Returns:\\n    float: The average of the sums of absolute differences for each shuffled permutation of the list.\\n\\n    Requirements:\\n    - itertools\\n    - random.shuffle\\n\\n    Example:\\n    >>> result = task_func([1, 2, 3])\\n    >>> isinstance(result, float)\\n    True\\n    \"\"\"\\n    permutations = list(itertools.permutations(numbers))\\n    total_sum = 0\\n    for perm in permutations:\\n        shuffled_perm = list(perm)\\n        shuffle(shuffled_perm)\\n        diff_sum = 0\\n        for i in range(len(shuffled_perm) - 1):\\n            diff_sum += abs(shuffled_perm[i] - shuffled_perm[i + 1])\\n        total_sum += diff_sum\\n    return total_sum / len(permutations)\\n\\nresult = task_func([1, 2, 3])\\nprint(result)\\n```\\n\\nThe completed code calculates the average of the sums of absolute differences between each pair of consecutive numbers for all permutations of a given list. Each permutation is shuffled before calculating the differences. The code uses the `itertools` module to generate all permutations of the input list and the `random.shuffle` function to shuffle each permutation. The average is then calculated by dividing the total sum of differences by the number of permutations.\\n\\nThe output of the code will be the average of the sums of absolute differences for each shuffled permutation of the list.'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/Users/andrew/Documents/01_documents/GWU_DENG/SEAS_8588_Praxis/2_Code/logs_results/final-round-one-model/BigCode/OpenCodeInterpreter-DS-6.7B/BigCode_OpenCodeInterpreter-DS-6.7B_complete_code_prompt_temperature1.0_topP1.0_completions_20250402_105939_8955.jsonl'\n",
    "results = list( stream_jsonl(file) )\n",
    "len(results), results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750908829761,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "hKReYoj4QIuI",
    "outputId": "933eedbe-ecad-478c-a805-999c0681a221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23500\n",
      "135165\n",
      "Total tokens in prompts for the entire experiment: 158665\n"
     ]
    }
   ],
   "source": [
    "# prompt template without content\n",
    "prompt_template_tokens = count_tokens(complete_code_prompt)*len(results)\n",
    "print(prompt_template_tokens)\n",
    "\n",
    "# prompt content\n",
    "file = 'data/Big_Code_Bench_Test.jsonl.gz'\n",
    "tasks = read_problems(file)\n",
    "prompt_content_tokens = count_tokens( ' '.join( [v['prompt'] for k,v in tasks.items()] ) )\n",
    "print(prompt_content_tokens)\n",
    "\n",
    "# total prompt tokens (entire experiment)\n",
    "total_prompt_tokens = prompt_template_tokens + prompt_content_tokens\n",
    "print('Total tokens in prompts for the entire experiment:', total_prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1750908535616,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "FLI99SL2Iuvo",
    "outputId": "868cb191-7553-4474-c78e-2e83f5e24b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated tokens for the entire experiment: 159818\n"
     ]
    }
   ],
   "source": [
    "# generated text\n",
    "generated_tokens = sum([count_tokens(i['completion']) for i in results])\n",
    "print('Total generated tokens for the entire experiment:', generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ijEMcpdcOTV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "319.636\n",
      "Average latency per one code generation: 5.753448 seconds\n"
     ]
    }
   ],
   "source": [
    "# average latency\n",
    "avg_tokens_per_one_generation  = generated_tokens/len(results)\n",
    "print( avg_tokens_per_one_generation )\n",
    "avg_latency_per_one_generation = avg_tokens_per_one_generation * gpt4_latency_per_token\n",
    "print(f'Average latency per one code generation: {avg_latency_per_one_generation} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYbotH-pUNSt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g4Y0DxfXyyv"
   },
   "source": [
    "## LBPP Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1750906647164,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "OR9TM0yaI9Cv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(162,\n",
       " {'task_id': 'lbpp/0',\n",
       "  'completion': '```python\\nimport numpy as np\\n\\ndef add_avg_and_std_cols_numpy(ar: np.ndarray) -> np.ndarray:\\n    avg_col = np.mean(ar, axis=1)\\n    std_col = np.std(ar, axis=1)\\n    return np.column_stack((ar, avg_col, std_col))\\n\\nar = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\nprint(add_avg_and_std_cols_numpy(ar))\\n\\nar = np.array([[1], [1], [2]])\\nprint(add_avg_and_std_cols_numpy(ar))\\n\\nar = np.array([[1, 2, 3, 4, 5]])\\nprint(add_avg_and_std_cols_numpy(ar))\\n```'})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/Users/andrew/Documents/01_documents/GWU_DENG/SEAS_8588_Praxis/2_Code/logs_results/final-round-one-model/LBPP/OpenCodeInterpreter-DS-6.7B/LBPP_OpenCodeInterpreter-DS-6.7B_complete_task_prompt_temperature1.0_topP1.0_completions_20250405_005934_6995.jsonl'\n",
    "results = list( stream_jsonl(file) )\n",
    "len(results), results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750908829761,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "hKReYoj4QIuI",
    "outputId": "933eedbe-ecad-478c-a805-999c0681a221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11340\n",
      "89246\n",
      "Total tokens in prompts for the entire experiment: 100586\n"
     ]
    }
   ],
   "source": [
    "# prompt template without content\n",
    "prompt_template_tokens = count_tokens( complete_task_prompt )*len(results)\n",
    "print(prompt_template_tokens)\n",
    "\n",
    "# prompt content\n",
    "file = 'data/LBPP_Test.jsonl.gz'\n",
    "tasks = read_problems(file)\n",
    "prompt_content_tokens1 = count_tokens( ' '.join( [v['prompt'] for k,v in tasks.items()] ) )\n",
    "prompt_content_tokens2 = count_tokens( ' '.join( [v['test'] for k,v in tasks.items()] ) )    # adding tests\n",
    "prompt_content_tokens = prompt_content_tokens1 + prompt_content_tokens2\n",
    "print(prompt_content_tokens)\n",
    "\n",
    "# total prompt tokens (entire experiment)\n",
    "total_prompt_tokens = prompt_template_tokens + prompt_content_tokens\n",
    "print('Total tokens in prompts for the entire experiment:', total_prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1750908535616,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "FLI99SL2Iuvo",
    "outputId": "868cb191-7553-4474-c78e-2e83f5e24b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated tokens for the entire experiment: 85263\n"
     ]
    }
   ],
   "source": [
    "# generated text\n",
    "generated_tokens = sum([count_tokens(i['completion']) for i in results])\n",
    "print('Total generated tokens for the entire experiment:', generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ijEMcpdcOTV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526.3148148148148\n",
      "Average latency per one code generation: 9.473666666666665 seconds\n"
     ]
    }
   ],
   "source": [
    "# average latency\n",
    "avg_tokens_per_one_generation  = generated_tokens/len(results)\n",
    "print( avg_tokens_per_one_generation )\n",
    "avg_latency_per_one_generation = avg_tokens_per_one_generation * gpt4_latency_per_token\n",
    "print(f'Average latency per one code generation: {avg_latency_per_one_generation} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGJjQTtAUNWJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8g4Y0DxfXyyv"
   },
   "source": [
    "## MBPP Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1750906647164,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "OR9TM0yaI9Cv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,\n",
       " {'task_id': '11',\n",
       "  'completion': '```python\\ndef remove_Occ(s, c):\\n    first_occurrence = s.find(c)\\n    last_occurrence = s.rfind(c)\\n    if first_occurrence == -1:\\n        return s\\n    elif first_occurrence == last_occurrence:\\n        return s[:first_occurrence] + s[first_occurrence+1:]\\n    else:\\n        return s[:first_occurrence] + s[first_occurrence+1:last_occurrence] + s[last_occurrence+1:]\\n\\n# Test cases\\nprint(remove_Occ(\"hello\",\"l\")) # Expected output: \"heo\"\\nprint(remove_Occ(\"abcda\",\"a\")) # Expected output: \"bcd\"\\nprint(remove_Occ(\"PHP\",\"P\")) # Expected output: \"H\"\\n```'})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = '/Users/andrew/Documents/01_documents/GWU_DENG/SEAS_8588_Praxis/2_Code/logs_results/final-round-one-model/MBPP/OpenCodeInterpreter-DS-6.7B/MBPP_OpenCodeInterpreter-DS-6.7B_complete_task_prompt_temperature1.0_topP1.0_completions_20250404_020543_0757.jsonl'\n",
    "results = list( stream_jsonl(file) )\n",
    "len(results), results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1750908829761,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "hKReYoj4QIuI",
    "outputId": "933eedbe-ecad-478c-a805-999c0681a221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "50790\n",
      "Total tokens in prompts for the entire experiment: 85790\n"
     ]
    }
   ],
   "source": [
    "# prompt template without content\n",
    "prompt_template_tokens = count_tokens( complete_task_prompt )*len(results)\n",
    "print(prompt_template_tokens)\n",
    "\n",
    "# prompt content\n",
    "file = 'data/MBPP_Test.jsonl.gz'\n",
    "tasks = read_problems(file)\n",
    "prompt_content_tokens1 = count_tokens( ' '.join( [v['prompt'] for k,v in tasks.items()] ) )\n",
    "prompt_content_tokens2 = count_tokens( ' '.join( [v['test'] for k,v in tasks.items()] ) )    # adding tests\n",
    "prompt_content_tokens = prompt_content_tokens1 + prompt_content_tokens2\n",
    "print(prompt_content_tokens)\n",
    "\n",
    "# total prompt tokens (entire experiment)\n",
    "total_prompt_tokens = prompt_template_tokens + prompt_content_tokens\n",
    "print('Total tokens in prompts for the entire experiment:', total_prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1750908535616,
     "user": {
      "displayName": "Andrew Nedilko",
      "userId": "10605440129054770392"
     },
     "user_tz": 420
    },
    "id": "FLI99SL2Iuvo",
    "outputId": "868cb191-7553-4474-c78e-2e83f5e24b65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total generated tokens for the entire experiment: 71265\n"
     ]
    }
   ],
   "source": [
    "# generated text\n",
    "generated_tokens = sum([count_tokens(i['completion']) for i in results])\n",
    "print('Total generated tokens for the entire experiment:', generated_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ijEMcpdcOTV5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.53\n",
      "Average latency per one code generation: 2.56554 seconds\n"
     ]
    }
   ],
   "source": [
    "# average latency\n",
    "avg_tokens_per_one_generation  = generated_tokens/len(results)\n",
    "print( avg_tokens_per_one_generation )\n",
    "avg_latency_per_one_generation = avg_tokens_per_one_generation * gpt4_latency_per_token\n",
    "print(f'Average latency per one code generation: {avg_latency_per_one_generation} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGJjQTtAUNWJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN2McfggFXmqSwq0biWjVBQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
