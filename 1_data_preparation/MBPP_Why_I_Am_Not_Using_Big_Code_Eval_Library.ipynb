{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d0a371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e32915a",
   "metadata": {},
   "source": [
    "# Big Code Evaluation Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3462ee5d",
   "metadata": {},
   "source": [
    "Name: bigcode-evaluation-harness  \n",
    "Website: https://github.com/bigcode-project/bigcode-evaluation-harness  \n",
    "Documentation: https://github.com/bigcode-project/bigcode-evaluation-harness/tree/main/docs\n",
    "\n",
    "!pip install bigcode-evaluation-harness  \n",
    "!pip install accelerate\n",
    "\n",
    "__Usage for MBPP (similar for HumanEval)__:  \n",
    "```\n",
    "accelerate launch  main.py \\\n",
    "  --model <MODEL_NAME> \\\n",
    "  --max_length_generation <MAX_LENGTH> \\\n",
    "  --tasks mbpp \\\n",
    "  --temperature 0.1 \\\n",
    "  --n_samples 15 \\\n",
    "  --batch_size 10 \\\n",
    "  --allow_code_execution\n",
    " ```\n",
    " \n",
    "__Convenient__ as you just point to the right HuggingFace model, right evaluation task (HumanEval or MBPP), indicate the required temperature, n_samples (samples to generate per each problem), etc. and run this.\n",
    "\n",
    "### Why I can't use this framework for my Praxis  \n",
    "I need to evaluate agents based on LLMs. __My agents will not be hosted on HuggingFace__ => I need custom code to evaluate first SLMs, and then SLM-based agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4c052f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1424cb0d",
   "metadata": {},
   "source": [
    "__Bigcode-evaluation-harness usage as generated by Gemini in Google search__\n",
    "\n",
    "`pip install bigcode-evaluation-harness`\n",
    "\n",
    "Generate solutions to evaluate:  \n",
    "Use the harness's generation capabilities or generate solutions separately and provide them in a JSON file.\n",
    "\n",
    "Evaluate:  \n",
    "`bigcode-eval evaluate mbpp --load_generations_path <your_generations.json> --n_samples <number_of_samples_per_problem>`\n",
    "\n",
    "<number_of_samples_per_problem> - number of solutions you generated per problem.\n",
    "\n",
    "Example:  \n",
    "`bigcode-eval evaluate mbpp --load_generations_path my_generations.json --n_samples 15`\n",
    "\n",
    "Explanation:\n",
    "* `bigcode-eval evaluate mbpp` tells the harness to evaluate on the MBPP benchmark.\n",
    "`--load_generations_path` specifies the path to the JSON file containing your model's generated solutions.\n",
    "`--n_samples` specifies the number of samples to evaluate per problem. \n",
    "Output:\n",
    "* The evaluation script will output the pass@1 score, which represents the percentage of problems your model solved correctly on the first try.\n",
    "* The harness supports different prompt types for MBPP, such as incoder and docstring. Choose the one that aligns with your model's training.\n",
    "* The evaluation is based on the provided test cases, which might not be exhaustive. Consider writing additional test cases to ensure thorough evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346fc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
