# Evaluating LLMs on the Code Generation Task

This repository contains the code and text for my doctoral dissertation (Praxis) - Boosting the Code Generation Capabilities of Small Language Models (SLMs) Using Agents.

## Evaluation of Code Generated by LLMs
* Notebook `Evaluate_Generated_Code_Using_Pass@1.ipynb` explains how the code generation capabilities of LLMs are evaluated. It loads the code generated by an LLM and uses the modified human-eval package to evaluate the code quality by computing the Pass@1 score. Pass@5 and Pass@10 scores can also be evaluated.
    * [Original human-eval package](https://github.com/openai/human-eval) - evaluates LLMs only on the [HumanEval](https://huggingface.co/datasets/openai/openai_humaneval) code generation dataset.
    * [Modified human-eval package](https://github.com/agnedil/code-generation/tree/main/modified-openai-human-eval-code) - I made an extensive addition to enable the evaluation on three more code generation datasets: [MBPP](https://huggingface.co/datasets/google-research-datasets/mbpp), [LBPP](https://huggingface.co/datasets/CohereForAI/lbpp), and [Big Code Benchmark](https://huggingface.co/datasets/bigcode/bigcodebench)

## Structure of the Repository

See:
* Notebook `Visualizing Code Generation Results for Single Models.ipynb` for code generation results visualization for single LLMs.
* Top directory for most of the code used in this research including:
    1. Preparing datasets for code generation evaluation,
    2. Running code generation experiments in Google Colab,
    3. Running code generation experiments locally using the `Mistral.ai` and `Replicate.com` APIs to make calls to LLMs. 
* Folder `logs` for all the logs generated during the evaluation of generated code, so far.
* Folder `documents` for all supporting documents used to write the Praxis along with the main Praxis document.
* Folder `modified-openai-human-eval-code` for the modified human-eval package used to evaluate code on four code generation datasets.